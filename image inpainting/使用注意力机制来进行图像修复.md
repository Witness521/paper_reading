### DeepFill v1 (Generative Image Inpainting with Contextual Attention)

**使用上下文注意力机制来进行深度图像修复**  [解读链接](https://mp.weixin.qq.com/s?__biz=Mzg5ODAzMTkyMg==&mid=2247494431&idx=1&sn=4556be2eb7d239e198f197e76d1761d0&chksm=c06a6342f71dea543422c787aa5a061b7bdba4a295923dab405a69fa53c1f10cffd2819d86d3&scene=21#wechat_redirect)

​    熟悉CNN的人应该知道，在卷积层，核的大小和膨胀率控制着感受野，网络需要越深入，才能看到整个输入图像。这意味着，如果我们想捕捉图像的上下文，我们必须依赖于更深的层次，但我们丢失了空间信息，因为更深层次的特征的空间大小总是更小。因此，我们必须找到一种方法，在不用太加深网络的情况下，从遥远的空间位置借用信息(即理解图像的上下文)。

![DeepFillv1.png](H:\files\python_file\paper_reading\image inpainting\pic\DeepFillv1.png)

对于该体系结构，所提出的框架由两个生成器网络和两个判别器网络组成。这两个生成器在全卷积网络的基础上使用了**膨胀卷积**。一个生成器用于**粗重建**，另一个用于**细化**。这被称为标准的从粗到细的网络结构。这两个判别器同时在全局和局部看完整的图像。**全局判别器**以整个图像作为输入，而**局部判别器**以填充区域作为输入。



<img src="H:\files\python_file\paper_reading\image inpainting\pic\空间注意力机制.png" alt="image-20220918115714664" style="zoom:70%;" />

​    以图4为例，生成的缺失区域内的特征大小为64×64×64，假设缺失区域外的特征分为128个小特征patch，大小为64×3×3。注意，本例中特征的通道大小是64。然后，我们将128个小的feature patch与缺失区域内生成的feature进行卷积，得到大小为**128×64×64**的feature map。其反映了128个patch对该位置的贡献，然后再进行Softmax归一化操作。

*与上一篇文章中提到的Shift-Net相比，你可以看到，这一次我们给每个已知特征的patch分配了权重，来表示重建的时候每个特征位置对于缺失区域的重要性（软分配），而不是对于缺失区域的每个位置找一个最相似的（硬分配）。这也是为什么提出的上下文注意力是可微的。*

**注意力传播**

注意力传播可以看作是注意特征图的微调。这里的关键思想是，**邻近的像素通常有更接近的像素值**。这意味着他们会考虑周围环境的注意力值来调整每个注意力分数。

![图片](https://mmbiz.qpic.cn/mmbiz_png/KYSDTmOVZvq9W45NyCMekglwCTBibG8vQAPVypH3ZzUlgPtwb8PUtbN3NFKLC5zVR3hiahPmiaToESHNeVCDWGx9g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

例如，如果我们考虑左邻居和右邻居的注意力值，我们可以使用上面列出的公式更新当前的注意力值。注意，*k*控制要考虑的邻居的数量。作者声称，这可以进一步提高修复结果，这也可以通过与单位矩阵卷积作为核来实现。

关于注意力机制的另一点是，采用了两种技术来控制提取的已知特征块的数量。

i) 以较大的步长提取已知的特征patch，以减少kernel数量。

ii) 操作前先对特征图大小进行向下采样，获取注意力图后再进行上采样。



**将注意力机制引入网络**

<img src="https://mmbiz.qpic.cn/mmbiz_png/KYSDTmOVZvq9W45NyCMekglwCTBibG8vQG5wNcerSh3yzK7LKrktXZstknw0j4EeJ9wEGjKKeaV61lniaEvB8Ozg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom:80%;" />

在细化网络中嵌入上下文注意力机制



### Interactive Image Inpainting Using Semantic Guidance (师兄的那篇)

<img src="H:\files\python_file\paper_reading\image inpainting\pic\师兄network.png" alt="image-20220919195814805" style="zoom:80%;" />

此篇论文中，ESPA Module使用的是外部空间注意力机制(**External Spatial Attetion Module**)，与DeepFill v1**相同点**都是将注意力机制的模块和扩张卷积并行，整合上下文和damaged图像的编码特征，以计算他们之间的长范围依赖。**不同点**是ESPA模块利用两个外部可学习的Key和Value分别与输入query在H维和W维度上相乘以实现空间信息的传播。
