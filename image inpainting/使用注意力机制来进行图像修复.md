### DeepFill v1 (Generative Image Inpainting with Contextual Attention)

**使用上下文注意力机制来进行深度图像修复**  [解读链接](https://mp.weixin.qq.com/s?__biz=Mzg5ODAzMTkyMg==&mid=2247494431&idx=1&sn=4556be2eb7d239e198f197e76d1761d0&chksm=c06a6342f71dea543422c787aa5a061b7bdba4a295923dab405a69fa53c1f10cffd2819d86d3&scene=21#wechat_redirect)

​    熟悉CNN的人应该知道，在卷积层，核的大小和膨胀率控制着感受野，网络需要越深入，才能看到整个输入图像。这意味着，如果我们想捕捉图像的上下文，我们必须依赖于更深的层次，但我们丢失了空间信息，因为更深层次的特征的空间大小总是更小。因此，我们必须找到一种方法，在不用太加深网络的情况下，从遥远的空间位置借用信息(即理解图像的上下文)。

![DeepFillv1.png](pic\DeepFillv1.png)

对于该体系结构，所提出的框架由两个生成器网络和两个判别器网络组成。这两个生成器在全卷积网络的基础上使用了**膨胀卷积**。一个生成器用于**粗重建**，另一个用于**细化**。这被称为标准的从粗到细的网络结构。这两个判别器同时在全局和局部看完整的图像。**全局判别器**以整个图像作为输入，而**局部判别器**以填充区域作为输入。

**注意力机制部分**    

以图4为例，生成的缺失区域内的特征大小为64×64×64，假设缺失区域外的特征分为128个小特征patch，大小为64×3×3。注意，本例中特征的通道大小是64。然后，我们将128个小的feature patch与缺失区域内生成的feature进行卷积，得到大小为**128×64×64**的feature map。其反映了128个patch对该位置的贡献，然后再进行Softmax归一化操作。

**在这张图中，最终得到的每一个像素的注意力得分通过反卷积（卷积核就是已知区域的patch）得到注意力机制作用后的图像。另一篇论文中则是通过得到的注意力得分与已知的背景patch相乘得到未知的前景图像（注：此方法可行的原因是前景已经不是空白区域，是已经经过卷积学习到特征的图像），师兄那篇则是通过输入的图片作为q与k相乘，得到注意力的权重。**



<img src="H:\files\python_file\paper_reading\image inpainting\pic\deepfillv1注意力反卷积.jpg" style="zoom: 67%;" />

*与上一篇文章中提到的Shift-Net相比，你可以看到，这一次我们给每个已知特征的patch分配了权重，来表示重建的时候每个特征位置对于缺失区域的重要性（软分配），而不是对于缺失区域的每个位置找一个最相似的（硬分配）。这也是为什么提出的上下文注意力是可微的。*

**注意力传播**

注意力传播可以看作是注意特征图的微调。这里的关键思想是，**邻近的像素通常有更接近的像素值**。这意味着他们会考虑周围环境的注意力值来调整每个注意力分数。

![图片](https://mmbiz.qpic.cn/mmbiz_png/KYSDTmOVZvq9W45NyCMekglwCTBibG8vQAPVypH3ZzUlgPtwb8PUtbN3NFKLC5zVR3hiahPmiaToESHNeVCDWGx9g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

例如，如果我们考虑左邻居和右邻居的注意力值，我们可以使用上面列出的公式更新当前的注意力值。注意，*k*控制要考虑的邻居的数量。作者声称，这可以进一步提高修复结果，这也可以通过与单位矩阵卷积作为核来实现。

关于注意力机制的另一点是，采用了两种技术来控制提取的已知特征块的数量。

i) 以较大的步长提取已知的特征patch，以减少kernel数量。

ii) 操作前先对特征图大小进行向下采样，获取注意力图后再进行上采样。



**将注意力机制引入网络**

<img src="https://mmbiz.qpic.cn/mmbiz_png/KYSDTmOVZvq9W45NyCMekglwCTBibG8vQG5wNcerSh3yzK7LKrktXZstknw0j4EeJ9wEGjKKeaV61lniaEvB8Ozg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom: 67%;" />

将使用注意力机制获得的特征图和扩张卷积获得特征图进行叠加



### DeepFill v2(Free-Form Image Inpainting with Gated Convolution)

**本文创新点：**门控卷积 gated conv：用可学习的卷积核来产生动态的软性mask

**整体网络结构图：**

<img src="H:\files\python_file\paper_reading\image inpainting\pic\deepfillv2.png" alt="image-20220927145216024" style="zoom: 50%;" />

**问题**：但是在inpainting问题中，浅层的网络中存在无效像素，也就是空洞区域的像素，深层网络中，虽然空洞可能因为填充而消失，但是填充像素也是合成的信息，并非原生的。这会**在目标区域是任意形状（free-form）的时候，在填充边缘周围造成缺陷**，比如颜色不一致、模糊和过度的边缘响应。

**解决方法**：

1. **partial conv**：Liu等人提出使用部分卷积(partial conv)的方法，部分卷积仅使用感受野范围内有效的像素参与计算，

- 当感受野范围内存在有效像素，则输出有效的卷积结果；
- 当感受野范围内全是无效像素，卷积结果则是无效像素。

​		partial conv方法总结：partial conv 的mask是不能主动学习的单通道的死板的硬门控（hard-gating）。而作者的gated conv则是一种能自主学习的多通道灵活的软门控。

​		**PCONV**：在门控卷积之前，要介绍**Partial Convolution部分卷积** [Pconv](https://zhuanlan.zhihu.com/p/519446359)。Pconv通过加入mask掩码到卷积运算中，大大提高了运算效率，并且将损坏与非损坏的区域区分开来。并且在每一步卷积之后，mask会被更新。

<img src="https://pic3.zhimg.com/80/v2-241768faced1ca82fa8722cf31c13f9e_720w.jpg" alt="img" style="zoom:80%;" />

部分卷积和普通卷积的区别，在部分卷积中，使用mask将需要修复的部分进行遮盖，然后再进行卷积，计算过程如下：

<img src="H:\files\python_file\paper_reading\image inpainting\pic\部分卷积.png" alt="image-20220926222300894" style="zoom: 67%;" />

在一次卷积操作结束之后，mask会被更新，因为部分invalid区域（原本M为0的区域）会被填充上元素。mask更新过程表示为：

<img src="https://pic2.zhimg.com/80/v2-871342c24dd3a445024cc160db7e8a75_720w.jpg" alt="img" style="zoom:80%;" />

当前运算3×3区域中，M只要有一个元素为1，那么更新后的 m′ （3×3的中心位置）就为1；反之仍然为0.

2. **门控卷积(gated conv)**

​		gated conv是本文的核心创新点，他就像一个软筛子一样，对输入有选择机制。（软性选择，就是乘以一个0-1之间的数，与之相对的是硬筛子，要么全通过，要么全拦下）。它可以自动根据更新规则从数据里面学到soft mask的参数（就像过滤系数一样），如下式：

![img](https://pic1.zhimg.com/80/v2-3bb0825f6c5f5617611617803586f600_720w.jpg)

Wg是作用在输入上以产生soft mask（就是gating）的卷积核，Wf就是常规的作用在输入上产生特征图的卷积核。 σ 是sigmoid函数，soft mask（门限值）经过sigmoid以后，将位于0-1之间； ϕ 就是卷积之后的激活函数（relu elu等）

<img src="https://pic1.zhimg.com/80/v2-be667b3be2fe76f52932926004f96808_720w.jpg" alt="img" style="zoom:67%;" />

门控卷积和普通卷积的对比，门控卷积使用soft-gating可以很好的针对每一个channel和每个空间位置，学习一种作用于feature map上的动态特征。

3. **SN-PatchGAN （Spectral Normalized Markovian Discriminator）**

​		作者借鉴了PatchGAN和谱归一化(Spectral Normalization, SN)，然后提出了简单有效的对抗机制：SN-PatchGAN，用于应对free-form的问题。

​		**patchGAN**又称Markovian discriminiator，由[pix2pix](https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2017/papers/Isola_Image-To-Image_Translation_With_CVPR_2017_paper.pdf)一文中提出使用。比较原始的GAN鉴别器，输入图片，输出一个标量值，判别图片是真还是假；而patchGAN输出的是一幅尺寸小于原图的N x N的矩阵，这时每个元素对应着原图的一片感受野，也就是给各感受野patch区域打分，故得名patchGAN。由于这是个全卷积网络（FCN也就是将分类网络的最后几层全连接层换成全卷积层，将图像级别的分类扩展到像素级别的分类），所以不受输入分辨率的限制。

​		**SN**：即谱归一化（Spectral Norm）可粗略理解为限制卷积层输出对输入的导数不超过1，来保证神经网络满足1-lipschitz连续性，一方面这是WGAN的要求，另一方面这使得参数变化也会更稳定，不容易出现梯度爆炸，保证网络训练更加稳定。

4. **损失函数**

​		作者在原文中说，用了SN-PatchGAN以后网络训练得比deepfillv1又稳又快，并且由于patchGAN本身编码了patch level的信息，所以并不需要感知损失（perceptual loss）。另外作者还提到，Liu Guilin的partial conv用了多达六项损失：两项重建损失、感知损失、两项风格损失、全变分损失。显然，这是一个弊端，因为选择权重超参数平衡各方是一件非常麻烦的事情。而本文的损失则比较精简，只有两项：**L1范数的重建损失和SN-patchGAN**两项损失，两项损失的权重超参也是很简单的一比一：

- L1范数的重建损失：即预测结果和真值的L1距离。作者没有说这个损失用在哪，不过按照惯性，应该是两个阶段都有使用。
- hinge loss 对抗损失：

<img src="H:\files\python_file\paper_reading\image inpainting\pic\SN-patchGAN loss.png" alt="image-20220927151819810" style="zoom: 67%;" />



### Interactive Image Inpainting Using Semantic Guidance (师兄的那篇)

<img src="pic\师兄network.png" alt="image-20220919195814805" style="zoom:80%;" />

此篇论文中，ESPA Module使用的是外部空间注意力机制(**External Spatial Attetion Module**)，与DeepFill v1**相同点**都是将注意力机制的模块和扩张卷积并行，整合上下文和damaged图像的编码特征，以计算他们之间的长范围依赖。**不同点**是ESPA模块利用两个外部可学习的Key和Value分别与输入query在H维和W维度上相乘以实现空间信息的传播。



### Image Inpainting Guided by Coherence Priors of Semantics and Textures

<img src="H:\files\python_file\paper_reading\image inpainting\pic\Coherence Priors of Semantics and Textures framework.png" alt="image-20220923112453769" style="zoom:80%;" />

CIM模块：再将encoder最后一层的编码特征送入decoder之前，先送入CIM模块提取特征（根据Framework来看，就是提取出图像的语义特征图，具体应结合代码看）





### 在图像上的Attention机制（整理）

注意力机制的本质就是定位到感兴趣的信息，抑制无用信息，结果通常都是以概率图或者概率特征向量的形式展示，从原理上来说，主要分为空间注意力模型，[通道](https://so.csdn.net/so/search?q=通道&spm=1001.2101.3001.7020)注意力模型，空间和通道混合注意力模型三种，这里不区分soft和hard attention。 [链接](https://blog.csdn.net/Vodka_Lou/article/details/115671748)

#### 1. 空间注意力模型(spatial attention)

  在二维平面上，对H x W尺寸的特征图学习到一个权重矩阵，对应每个像素都会学习到一个权重。而这些**权重**代表的就是**某个空间位置信息的重要程度** ，将该空间注意力矩阵附加在原来的特征图上，增大有用的特征，弱化无用特征，从而起到特征筛选和增强的效果。代表的Self-Attention、Non-local Attention以及Spatial Transformer等。

- ##### Self-Attention （自注意力机制）

(1) 第一步是将query和每个key进行相似度计算得到权重，常用的相似度函数有点积，拼接，感知机等；

(2) 第二步一般是使用一个softmax函数对这些权重进行归一化，转换为注意力；

(3) 第三步将权重和相应的键值value进行加权求和得到最后的attention。

<img src="pic\selfAttention.png" alt="image-20220920110348618" style="zoom:80%;" />

- **Non-local Attention（非局部注意力）**

  CNN中的卷积单元每次只关注邻域kernel size 的区域，就算后期感受野越来越大，终究还是局部区域的运算，这样就忽略了全局其他片区（比如很远的像素）对当前区域的贡献。所以Non-local blocks 要做的是，捕获这种**long-range** 关系：对于2D图像，就是图像中任何像素对当前像素的关系权值；对于3D视频，就是所有帧中的所有像素，对当前帧的像素的关系权值。

<img src="https://img-blog.csdnimg.cn/img_convert/a1268f85ba2cd7df15a5c22275e42945.png" alt="img" style="zoom:80%;" />

#### 2. 通道注意力模型（channel Attention）

​    不同于空间注意力机制给每一个像素都学习到一个权重，通道注意力给每个**通道(channel)**上的特征图学习一个**权重**，代表该通道与关键信息的相似度。在神经网络中，feature map尺寸越小，channel数量越多，这时如果用一个通道注意力告诉该网络哪些是重要的，往往能起到很好的效果，这时CV领域做通道注意力往往比空间好的一个原因。代表的是SENet、SKNet、ECANet等。

- **SENet** [知乎链接](https://zhuanlan.zhihu.com/p/32702350)

  对于卷积操作，很大一部分工作是提高感受野，即空间上融合更多特征融合，或者是提取多尺度空间信息。而SENet网络的创新点在于关注**channel之间的关系**，希望模型可以自动学习到不同channel特征的重要程度。为此，SENet提出了Squeeze-and-Excitation (SE)模块

![img](https://pic1.zhimg.com/80/v2-eb33a772a6029e5c8011a5ab77ea2f74_720w.jpg)

图中的$F_{tr}$ 是传统的卷积结构，SENet增加的是U后的内容，对U先做一个**全局平均池化**（文中称为squeeze），输出的1x1xC数据再经过**两个全连接层**（文中称为Excitation），最后用**sigmoid**（文中称为self-gating mechanism）把这个值作为scale乘到U的C个通道上，作为下一级的输入数据。

SENet是在channel维度上做attention或者gating操作，这种注意力机制可以让模型关注信息量最大的channel特征，而抑制那些不重要的channel特征

在Excitation中，为了降低模型复杂度以及提升泛化能力，这里采用包含两个全连接层的bottleneck结构，其中第一个FC层起到降维的作用，降维系数为r是个超参数，然后采用ReLU激活。最后的FC层恢复原始的维度。

<img src="https://pic3.zhimg.com/80/v2-8515d83936b780c200f62caf9ee37212_720w.jpg" alt="img" style="zoom:80%;" />

模型可以嵌入到Inception和ResNet中



- **SKNet**

SKNet是基于SENet的改进，他的思路是在提高精度。

![img](https://img-blog.csdnimg.cn/img_convert/80b5d3fce7ede2e32116439c1d4a398b.png)

上图所示是SKNet的基本结构。主要创新点是设置了一组动态选择的卷积，分为三个部分操作Split、Fuse、Select。

（1）Split：对输入向量X进行不同卷积核大小的完整卷积操作（组卷积），特别地，为了进一步提升效率，将5x5的传统卷积替代为dilation=2，卷积核为3x3的空洞卷积；

（2）Fuse：类似SE模块的处理，两个feature map相加后，进行全局平均池化操作，全连接先降维再升维的为两层全连接层，输出的两个注意力系数向量a和b，其中a+b=1；

（3）Select：Select操作对应于SE模块中的Scale。Select使用a和b两个权重矩阵对之前的两个feature map进行加权操作，它们之间有一个类似于特征挑选的操作。



#### 3. 混合域注意力方法

空间注意力和通道注意力结合，根据DL任务的不同，它们结合方式也存在区别，有代表性的是CBAM、DANet、CCNet、Residual Attention等

- **CBAM**（效果很不错）ECCV2018

<img src="https://img-blog.csdnimg.cn/img_convert/2321e58816ef41db554721667f22a022.png" alt="img" style="zoom:80%;" />

CBDM的基本结构，前面使用SEnet的通道注意力模块，后面的注意力模块设计也参考了SENet，它将全局平均池化用在了通道上，因此作用后就得到了一个二维的空间注意力系数矩阵。CBAM在通道与空间上同时做全局平均和全局最大的混合Pooling，能提取到更多有效信息。

作者将注意力过程分为两个独立的部分，通道注意力模块和空间注意力模块。这样不仅可以节约参数和计算力，而且保证了其可以作为即插即用的模块集成到现有的网络架构中去。

1. **通道的注意力模块(Channel Attention Module)**

<img src="https://pic2.zhimg.com/80/v2-8fefab2340c4a045f77ad35501717e89_720w.jpg" alt="img" style="zoom:80%;" />

如上图所示，输入是一个 H×W×C 的特征 F，我们先分别进行一个空间的全局平均池化和最大池化得到两个 1×1×C 的通道描述。接着，再将它们分别送入一个两层的神经网络，第一层神经元个数为 C/r，激活函数为 Relu，第二层神经元个数为 C。注意，这个两层的神经网络是共享的。

然后，再将得到的两个特征相加后经过一个 Sigmoid 激活函数得到权重系数 Mc。最后，拿权重系数和原来的特征 F 相乘即可得到缩放后的新特征。

2. **空间注意力模块**

<img src="https://pic3.zhimg.com/80/v2-e2b0976b0ed30ad554abfd80a912407a_720w.jpg" alt="img" style="zoom:80%;" />

与通道注意力相似，给定一个 H×W×C 的特征 F‘，我们先分别进行一个通道维度的平均池化和最大池化得到两个 H×W×1 的通道描述，并将这两个描述按照通道拼接在一起。然后，经过一个 7×7 的卷积层（此处应该是same卷积，即卷积完特征图的大小不变），激活函数为 Sigmoid，得到权重系数Ms。最后，拿权重系数和特征 F’ 相乘即可得到缩放后的新特征。

<img src="https://pic3.zhimg.com/80/v2-c023d0d068bca35204040fe4025471da_720w.jpg" alt="img" style="zoom:80%;" />

最后将通道注意力和空间注意力采用顺序的方式存放。



- DANet CVPR2019  [知乎](https://zhuanlan.zhihu.com/p/54510782)

<img src="https://pic2.zhimg.com/80/v2-9aa99faf0d6b14947100f8536c339525_720w.jpg" alt="img" style="zoom:80%;" />

DANet整体框架图

1. **Position Attention Module(空间上的注意力模块)**

<img src="https://pic4.zhimg.com/80/v2-09f16addd1f4666f476b029c40427aab_720w.jpg" alt="img" style="zoom:80%;" />

- 特征图**A**(C×H×W)首先分别通过3个卷积层得到3个特征图**B,C,D,**然后将**B,C,D** reshape为C×N，其中N=H×W
- 之后将reshape后的**B**的转置(NxC)与reshape后的C(CxN)相乘，再通过softmax得到spatial attention map **S**(N×N)
- 接着在reshape后的**D**(CxN)和**S**的转置(NxN)之间执行矩阵乘法，再乘以尺度系数α，再reshape为原来形状，最后与**A**相加得到最后的输出**E**
- 其中α初始化为0，并逐渐的学习得到更大的权重

`在空间上做Attention module是将W和H乘起来作为N，在N上进行注意力的计算，如果是在channel上做的话,则是保留下C*C作为注意力矩阵`

2. **Channel Attention Module**

<img src="https://pic3.zhimg.com/80/v2-b282a763dc1b62cc0615c0a2a8e0beba_720w.jpg" alt="img" style="zoom:80%;" />

- 分别对**A**做reshape(CxN)和reshape与transpose(NxC)
- 将得到的两个特征图相乘，再通过softmax得到channel attention map **X**(C×C)
- 接着把**X**的转置(CxC)与reshape的**A**(CxN)做矩阵乘法，再乘以尺度系数β，再reshape为原来形状，最后与**A**相加得到最后的输出**E**
- 其中β初始化为0，并逐渐的学习得到更大的权重

