### 1. DeepFill v1 (Generative Image Inpainting with Contextual Attention)

**使用上下文注意力机制来进行深度图像修复**  [解读链接](https://mp.weixin.qq.com/s?__biz=Mzg5ODAzMTkyMg==&mid=2247494431&idx=1&sn=4556be2eb7d239e198f197e76d1761d0&chksm=c06a6342f71dea543422c787aa5a061b7bdba4a295923dab405a69fa53c1f10cffd2819d86d3&scene=21#wechat_redirect)

​    熟悉CNN的人应该知道，在卷积层，核的大小和膨胀率控制着感受野，网络需要越深入，才能看到整个输入图像。这意味着，如果我们想捕捉图像的上下文，我们必须依赖于更深的层次，但我们丢失了空间信息，因为更深层次的特征的空间大小总是更小。因此，我们必须找到一种方法，在不用太加深网络的情况下，从遥远的空间位置借用信息(即理解图像的上下文)。

![DeepFillv1.png](pic\DeepFillv1.png)

对于该体系结构，所提出的框架由两个生成器网络和两个判别器网络组成。这两个生成器在全卷积网络的基础上使用了**膨胀卷积**。一个生成器用于**粗重建**，另一个用于**细化**。这被称为标准的从粗到细的网络结构。这两个判别器同时在全局和局部看完整的图像。**全局判别器**以整个图像作为输入，而**局部判别器**以填充区域作为输入。

**注意力机制部分**    

以图4为例，生成的缺失区域内的特征大小为64×64×64，假设缺失区域外的特征分为128个小特征patch，大小为64×3×3。注意，本例中特征的通道大小是64。然后，我们将128个小的feature patch与缺失区域内生成的feature进行卷积，得到大小为**128×64×64**的feature map。其反映了128个patch对该位置的贡献，然后再进行Softmax归一化操作。

**在这张图中，最终得到的每一个像素的注意力得分通过反卷积（卷积核就是已知区域的patch）得到注意力机制作用后的图像。另一篇论文中则是通过得到的注意力得分与已知的背景patch相乘得到未知的前景图像（注：此方法可行的原因是前景已经不是空白区域，是已经经过卷积学习到特征的图像），师兄那篇则是通过输入的图片作为q与k相乘，得到注意力的权重。**



<img src="pic\deepfillv1注意力反卷积.jpg" style="zoom: 67%;" />

*与上一篇文章中提到的Shift-Net相比，你可以看到，这一次我们给每个已知特征的patch分配了权重，来表示重建的时候每个特征位置对于缺失区域的重要性（软分配），而不是对于缺失区域的每个位置找一个最相似的（硬分配）。这也是为什么提出的上下文注意力是可微的。*

**注意力传播**

注意力传播可以看作是注意特征图的微调。这里的关键思想是，**邻近的像素通常有更接近的像素值**。这意味着他们会考虑周围环境的注意力值来调整每个注意力分数。

![图片](https://mmbiz.qpic.cn/mmbiz_png/KYSDTmOVZvq9W45NyCMekglwCTBibG8vQAPVypH3ZzUlgPtwb8PUtbN3NFKLC5zVR3hiahPmiaToESHNeVCDWGx9g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

例如，如果我们考虑左邻居和右邻居的注意力值，我们可以使用上面列出的公式更新当前的注意力值。注意，*k*控制要考虑的邻居的数量。作者声称，这可以进一步提高修复结果，这也可以通过与单位矩阵卷积作为核来实现。

关于注意力机制的另一点是，采用了两种技术来控制提取的已知特征块的数量。

i) 以较大的步长提取已知的特征patch，以减少kernel数量。

ii) 操作前先对特征图大小进行向下采样，获取注意力图后再进行上采样。



**将注意力机制引入网络**

<img src="https://mmbiz.qpic.cn/mmbiz_png/KYSDTmOVZvq9W45NyCMekglwCTBibG8vQG5wNcerSh3yzK7LKrktXZstknw0j4EeJ9wEGjKKeaV61lniaEvB8Ozg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom: 67%;" />

将使用注意力机制获得的特征图和扩张卷积获得特征图进行叠加



### 2. DeepFill v2(Free-Form Image Inpainting with Gated Convolution)

**本文创新点：**门控卷积 gated conv：用可学习的卷积核来产生动态的软性mask

**整体网络结构图：**

<img src="pic\deepfillv2.png" alt="image-20220927145216024" style="zoom: 50%;" />

**问题**：但是在inpainting问题中，浅层的网络中存在无效像素，也就是空洞区域的像素，深层网络中，虽然空洞可能因为填充而消失，但是填充像素也是合成的信息，并非原生的。这会**在目标区域是任意形状（free-form）的时候，在填充边缘周围造成缺陷**，比如颜色不一致、模糊和过度的边缘响应。

**解决方法**：

1. **partial conv**：Liu等人提出使用部分卷积(partial conv)的方法，部分卷积仅使用感受野范围内有效的像素参与计算，

- 当感受野范围内存在有效像素，则输出有效的卷积结果；
- 当感受野范围内全是无效像素，卷积结果则是无效像素。

​		partial conv方法总结：partial conv 的mask是不能主动学习的单通道的死板的硬门控（hard-gating）。而作者的gated conv则是一种能自主学习的多通道灵活的软门控。

​		**PCONV**：在门控卷积之前，要介绍**Partial Convolution部分卷积** [Pconv](https://zhuanlan.zhihu.com/p/519446359)。Pconv通过加入mask掩码到卷积运算中，大大提高了运算效率，并且将损坏与非损坏的区域区分开来。并且在每一步卷积之后，mask会被更新。

<img src="https://pic3.zhimg.com/80/v2-241768faced1ca82fa8722cf31c13f9e_720w.jpg" alt="img" style="zoom:80%;" />

部分卷积和普通卷积的区别，在部分卷积中，使用mask将需要修复的部分进行遮盖，然后再进行卷积，计算过程如下：

<img src="pic\部分卷积.png" alt="image-20220926222300894" style="zoom: 67%;" />

在一次卷积操作结束之后，mask会被更新，因为部分invalid区域（原本M为0的区域）会被填充上元素。mask更新过程表示为：

<img src="https://pic2.zhimg.com/80/v2-871342c24dd3a445024cc160db7e8a75_720w.jpg" alt="img" style="zoom:80%;" />

当前运算3×3区域中，M只要有一个元素为1，那么更新后的 m′ （3×3的中心位置）就为1；反之仍然为0.

2. **门控卷积(gated conv)**

​		gated conv是本文的核心创新点，他就像一个软筛子一样，对输入有选择机制。（软性选择，就是乘以一个0-1之间的数，与之相对的是硬筛子，要么全通过，要么全拦下）。它可以自动根据更新规则从数据里面学到soft mask的参数（就像过滤系数一样），如下式：

![img](https://pic1.zhimg.com/80/v2-3bb0825f6c5f5617611617803586f600_720w.jpg)

Wg是作用在输入上以产生soft mask（就是gating）的卷积核，Wf就是常规的作用在输入上产生特征图的卷积核。 σ 是sigmoid函数，soft mask（门限值）经过sigmoid以后，将位于0-1之间； ϕ 就是卷积之后的激活函数（relu elu等）

<img src="https://pic1.zhimg.com/80/v2-be667b3be2fe76f52932926004f96808_720w.jpg" alt="img" style="zoom:67%;" />

门控卷积和普通卷积的对比，门控卷积使用soft-gating可以很好的针对每一个channel和每个空间位置，学习一种作用于feature map上的动态特征。

3. **SN-PatchGAN （Spectral Normalized Markovian Discriminator）**

​		作者借鉴了PatchGAN和谱归一化(Spectral Normalization, SN)，然后提出了简单有效的对抗机制：SN-PatchGAN，用于应对free-form的问题。

​		**patchGAN**又称Markovian discriminiator，由[pix2pix](https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2017/papers/Isola_Image-To-Image_Translation_With_CVPR_2017_paper.pdf)一文中提出使用。比较原始的GAN鉴别器，输入图片，输出一个标量值，判别图片是真还是假；而patchGAN输出的是一幅尺寸小于原图的N x N的矩阵，这时每个元素对应着原图的一片感受野，也就是给各感受野patch区域打分，故得名patchGAN。由于这是个全卷积网络（FCN也就是将分类网络的最后几层全连接层换成全卷积层，将图像级别的分类扩展到像素级别的分类），所以不受输入分辨率的限制。

​		**SN**：即谱归一化（Spectral Norm）可粗略理解为限制卷积层输出对输入的导数不超过1，来保证神经网络满足1-lipschitz连续性，一方面这是WGAN的要求，另一方面这使得参数变化也会更稳定，不容易出现梯度爆炸，保证网络训练更加稳定。

4. **损失函数**

​		作者在原文中说，用了SN-PatchGAN以后网络训练得比deepfillv1又稳又快，并且由于patchGAN本身编码了patch level的信息，所以并不需要感知损失（perceptual loss）。另外作者还提到，Liu Guilin的partial conv用了多达六项损失：两项重建损失、感知损失、两项风格损失、全变分损失。显然，这是一个弊端，因为选择权重超参数平衡各方是一件非常麻烦的事情。而本文的损失则比较精简，只有两项：**L1范数的重建损失和SN-patchGAN**两项损失，两项损失的权重超参也是很简单的一比一：

- L1范数的重建损失：即预测结果和真值的L1距离。作者没有说这个损失用在哪，不过按照惯性，应该是两个阶段都有使用。
- hinge loss 对抗损失：

<img src="pic\SN-patchGAN loss.png" alt="image-20220927151819810" style="zoom: 67%;" />



### 3. Interactive Image Inpainting Using Semantic Guidance (师兄的那篇)

<img src="pic\师兄network.png" alt="image-20220919195814805" style="zoom:80%;" />

此篇论文中，ESPA Module使用的是外部空间注意力机制(**External Spatial Attetion Module**)，与DeepFill v1**相同点**都是将注意力机制的模块和扩张卷积并行，整合上下文和damaged图像的编码特征，以计算他们之间的长范围依赖。**不同点**是ESPA模块利用两个外部可学习的Key和Value分别与输入query在H维和W维度上相乘以实现空间信息的传播。

#### 	1. AdaIN

​		AdaIN的主要目标就是实现实时的、任意风格的风格迁移，主要方法就是**自适应实例标准化**（Adaptive Instance Normalization，AdaIN）将内容图像（content image）特征的均值和方差对齐到风格图像（style image）的均值和方差

​		在训练过程中，引入Batch Normalization(BN)可以大大简化向前神经网络的训练，BN的作用就是将数据进行标准化（每一个值减去batch的均值，除以batch的标准差）。 将BN替换为Instance Normalization（IN），可以提升风格迁移的性能。IN的操作跟BN类似，就是范围从一个batch变成了一个instance。**因此作者提出，instance normalization通过对特征的统计特性（均值和方差）进行标准化，实现了某种形式的风格标准化（style normalization）。 特征的均值和方差就代表着图像的风格**

​		在BN，IN，CIN中，网络会学习仿射变换参数γ 和 β，作者提出的AdaIN则无需学习这两个参数，直接用style image的特征的均值和标准差代替这两个参数，所使用的公式如下： [链接](https://zhuanlan.zhihu.com/p/158657861)

<img src="https://pic1.zhimg.com/80/v2-d22f6ed1713dcecf412be9b4aa58bff4_720w.webp" alt="img" style="zoom:80%;" />

其中， μ(x) 和 σ(x) 分别表示content image的特征的均值和标准差，μ(y) 和 σ(y) 分别表示style image的特征的均值和标准差。这个公式可以理解为，先去风格化（减去自身均值再除以自身标准差），再风格化到style image的风格（乘style image的标准差再加均值 ）。

<img src="pic\image-20221007200730071.png" alt="image-20221007200730071" style="zoom:50%;" />

先使用VGG提取content image和style image中的特征，然后在AdaIN中进行上式的风格转换，再通过VGG将特征还原成图像，最后再计算content loss和style loss，VGG的参数在训练过程中是不更新的，训练的目的是为了得到一个好的Decoder。



#### 	2. SPADE模型

​		spade模型完成的是**语义图像合成**的任务，其是指基于语义分割的结果来生成真实图片，生成的真实图片具有结果的多样性。目前语义图像合成领域的经典方法有：CRN [1], pix2pixHD [2], SIMS [3], SPADE [4]。以往深度学习的方法经常将语义图直接送入神经网络进行学习，但对于高质量图片的生成存在一定的问题：在普通神经网络中的归一化层倾向于“洗去”(wash away)语义信息。为了解决这一问题，提出了**空间自适应归一化的语义合成方法**(spade)。

<img src="pic\image-20221007145229936.png" alt="image-20221007145229936" style="zoom:80%;" />

**SPADE generator**整体结构 

​		在左边spade的输入不是完整的图片，而是先生成一列学习好的数据分布，然后通过一层一层的SPADE ResBlk堆叠而成，feature map尺寸由小到大，通道数由大到小来生成最终的真实图片的。而在每一层SPADE ResBlk中，不断地加入**语义分割图片**来进行干预，这样可以让网络在每一层都能学习到多尺度的语义信息。

<img src="pic\image-20221007160320904.png" alt="image-20221007160320904" style="zoom:80%;" />         <img src="pic\image-20221007151051382.png" alt="image-20221007151051382"  />

每一个SPADE ResBLK模块都是由spade模块加上残差链接组成的，作者将我们常用的“卷积→激活→归一化”模块替换成了"SPADE→激活→卷积"，SPADE模块可以看作是利用**语义图信息来指导**feature map进行**归一化**。

也就是意味这，使用spade替代普通的normalization，不仅可以保留住归一化的功能，还可以更好地保留住初始地语义信息。

<img src="pic\image-20221007160653228.png" alt="image-20221007160653228" style="zoom: 67%;" />

针对在**训练过程中**学习好的分布数据，是通过Image Encoder学习到的，也就是先将真实图片标签送入Image Encoder中，得到其图片的均值和方差，再加上随机数来生成一个标准的分布数据。

<img src="pic\image-20221008161702409.png" alt="image-20221008161702409" style="zoom:60%;" />

训练过程图

<img src="pic\image-20221007161427965.png" alt="image-20221007161427965" style="zoom: 80%;" />

**通过对代码进行调试得出的结论是**：在使用spade预训练模型的时候，只需输入一个语义分割网络，不需要输入正态分布的噪声（这个输入由对**语义分割网络**进行上采样和卷积等操作得到）。

<img src="pic\image-20221008230226852.png" alt="image-20221008230226852" style="zoom: 67%;" />

但是，想要在使用预训练模型的时候通过--use_vae来将image Encoder中的信息输入Generator会产生问题(模型中fc的参数和预训练模型中的参数对不上) 因此**猜想**是只能在train的过程中使用Image Encoder，后续还需要进一步的验证(10.8)



### 4. Image Inpainting Guided by Coherence Priors of Sem antics and Textures

<img src="pic\Coherence Priors of Semantics and Textures framework.png" alt="image-20220923112453769" style="zoom:80%;" />

CIM模块：再将encoder最后一层的编码特征送入decoder之前，先送入CIM模块提取特征（根据Framework来看，就是提取出图像的语义特征图，具体应结合代码看）





### 5. 在图像上的Attention机制（整理）

注意力机制的本质就是定位到感兴趣的信息，抑制无用信息，结果通常都是以概率图或者概率特征向量的形式展示，从原理上来说，主要分为空间注意力模型，[通道](https://so.csdn.net/so/search?q=通道&spm=1001.2101.3001.7020)注意力模型，空间和通道混合注意力模型三种，这里不区分soft和hard attention。 [链接](https://blog.csdn.net/Vodka_Lou/article/details/115671748)

#### 1. 空间注意力模型(spatial attention)

  在二维平面上，对H x W尺寸的特征图学习到一个权重矩阵，对应每个像素都会学习到一个权重。而这些**权重**代表的就是**某个空间位置信息的重要程度** ，将该空间注意力矩阵附加在原来的特征图上，增大有用的特征，弱化无用特征，从而起到特征筛选和增强的效果。代表的Self-Attention、Non-local Attention以及Spatial Transformer等。

- ##### Self-Attention （自注意力机制）

(1) 第一步是将query和每个key进行相似度计算得到权重，常用的相似度函数有点积，拼接，感知机等；

(2) 第二步一般是使用一个softmax函数对这些权重进行归一化，转换为注意力；

(3) 第三步将权重和相应的键值value进行加权求和得到最后的attention。

<img src="pic\selfAttention.png" alt="image-20220920110348618" style="zoom:80%;" />

- **Non-local Attention（非局部注意力）**

  CNN中的卷积单元每次只关注邻域kernel size 的区域，就算后期感受野越来越大，终究还是局部区域的运算，这样就忽略了全局其他片区（比如很远的像素）对当前区域的贡献。所以Non-local blocks 要做的是，捕获这种**long-range** 关系：对于2D图像，就是图像中任何像素对当前像素的关系权值；对于3D视频，就是所有帧中的所有像素，对当前帧的像素的关系权值。

<img src="https://img-blog.csdnimg.cn/img_convert/a1268f85ba2cd7df15a5c22275e42945.png" alt="img" style="zoom:80%;" />

#### 2. 通道注意力模型（channel Attention）

​    不同于空间注意力机制给每一个像素都学习到一个权重，通道注意力给每个**通道(channel)**上的特征图学习一个**权重**，代表该通道与关键信息的相似度。在神经网络中，feature map尺寸越小，channel数量越多，这时如果用一个通道注意力告诉该网络哪些是重要的，往往能起到很好的效果，这时CV领域做通道注意力往往比空间好的一个原因。代表的是SENet、SKNet、ECANet等。

- **SENet** [知乎链接](https://zhuanlan.zhihu.com/p/32702350)

  对于卷积操作，很大一部分工作是提高感受野，即空间上融合更多特征融合，或者是提取多尺度空间信息。而SENet网络的创新点在于关注**channel之间的关系**，希望模型可以自动学习到不同channel特征的重要程度。为此，SENet提出了Squeeze-and-Excitation (SE)模块

![img](https://pic1.zhimg.com/80/v2-eb33a772a6029e5c8011a5ab77ea2f74_720w.jpg)

图中的$F_{tr}$ 是传统的卷积结构，SENet增加的是U后的内容，对U先做一个**全局平均池化**（文中称为squeeze），输出的1x1xC数据再经过**两个全连接层**（文中称为Excitation），最后用**sigmoid**（文中称为self-gating mechanism）把这个值作为scale乘到U的C个通道上，作为下一级的输入数据。

SENet是在channel维度上做attention或者gating操作，这种注意力机制可以让模型关注信息量最大的channel特征，而抑制那些不重要的channel特征

在Excitation中，为了降低模型复杂度以及提升泛化能力，这里采用包含两个全连接层的bottleneck结构，其中第一个FC层起到降维的作用，降维系数为r是个超参数，然后采用ReLU激活。最后的FC层恢复原始的维度。

<img src="https://pic3.zhimg.com/80/v2-8515d83936b780c200f62caf9ee37212_720w.jpg" alt="img" style="zoom:80%;" />

模型可以嵌入到Inception和ResNet中



- **SKNet**

SKNet是基于SENet的改进，他的思路是在提高精度。

![img](https://img-blog.csdnimg.cn/img_convert/80b5d3fce7ede2e32116439c1d4a398b.png)

上图所示是SKNet的基本结构。主要创新点是设置了一组动态选择的卷积，分为三个部分操作Split、Fuse、Select。

（1）Split：对输入向量X进行不同卷积核大小的完整卷积操作（组卷积），特别地，为了进一步提升效率，将5x5的传统卷积替代为dilation=2，卷积核为3x3的空洞卷积；

（2）Fuse：类似SE模块的处理，两个feature map相加后，进行全局平均池化操作，全连接先降维再升维的为两层全连接层，输出的两个注意力系数向量a和b，其中a+b=1；

（3）Select：Select操作对应于SE模块中的Scale。Select使用a和b两个权重矩阵对之前的两个feature map进行加权操作，它们之间有一个类似于特征挑选的操作。



#### 3. 混合域注意力方法

空间注意力和通道注意力结合，根据DL任务的不同，它们结合方式也存在区别，有代表性的是CBAM、DANet、CCNet、Residual Attention等

- **CBAM**（效果很不错）ECCV2018

<img src="https://img-blog.csdnimg.cn/img_convert/2321e58816ef41db554721667f22a022.png" alt="img" style="zoom:80%;" />

CBDM的基本结构，前面使用SEnet的通道注意力模块，后面的注意力模块设计也参考了SENet，它将全局平均池化用在了通道上，因此作用后就得到了一个二维的空间注意力系数矩阵。CBAM在通道与空间上同时做全局平均和全局最大的混合Pooling，能提取到更多有效信息。

作者将注意力过程分为两个独立的部分，通道注意力模块和空间注意力模块。这样不仅可以节约参数和计算力，而且保证了其可以作为即插即用的模块集成到现有的网络架构中去。

1. **通道的注意力模块(Channel Attention Module)**

<img src="https://pic2.zhimg.com/80/v2-8fefab2340c4a045f77ad35501717e89_720w.jpg" alt="img" style="zoom:80%;" />

如上图所示，输入是一个 H×W×C 的特征 F，我们先分别进行一个空间的全局平均池化和最大池化得到两个 1×1×C 的通道描述。接着，再将它们分别送入一个两层的神经网络，第一层神经元个数为 C/r，激活函数为 Relu，第二层神经元个数为 C。注意，这个两层的神经网络是共享的。

然后，再将得到的两个特征相加后经过一个 Sigmoid 激活函数得到权重系数 Mc。最后，拿权重系数和原来的特征 F 相乘即可得到缩放后的新特征。

2. **空间注意力模块**

<img src="https://pic3.zhimg.com/80/v2-e2b0976b0ed30ad554abfd80a912407a_720w.jpg" alt="img" style="zoom:80%;" />

与通道注意力相似，给定一个 H×W×C 的特征 F‘，我们先分别进行一个通道维度的平均池化和最大池化得到两个 H×W×1 的通道描述，并将这两个描述按照通道拼接在一起。然后，经过一个 7×7 的卷积层（此处应该是same卷积，即卷积完特征图的大小不变），激活函数为 Sigmoid，得到权重系数Ms。最后，拿权重系数和特征 F’ 相乘即可得到缩放后的新特征。

<img src="https://pic3.zhimg.com/80/v2-c023d0d068bca35204040fe4025471da_720w.jpg" alt="img" style="zoom:80%;" />

最后将通道注意力和空间注意力采用顺序的方式存放。



- DANet CVPR2019  [知乎](https://zhuanlan.zhihu.com/p/54510782)

<img src="https://pic2.zhimg.com/80/v2-9aa99faf0d6b14947100f8536c339525_720w.jpg" alt="img" style="zoom:80%;" />

DANet整体框架图

1. **Position Attention Module(空间上的注意力模块)**

<img src="https://pic4.zhimg.com/80/v2-09f16addd1f4666f476b029c40427aab_720w.jpg" alt="img" style="zoom:80%;" />

- 特征图**A**(C×H×W)首先分别通过3个卷积层得到3个特征图**B,C,D,**然后将**B,C,D** reshape为C×N，其中N=H×W
- 之后将reshape后的**B**的转置(NxC)与reshape后的C(CxN)相乘，再通过softmax得到spatial attention map **S**(N×N)
- 接着在reshape后的**D**(CxN)和**S**的转置(NxN)之间执行矩阵乘法，再乘以尺度系数α，再reshape为原来形状，最后与**A**相加得到最后的输出**E**
- 其中α初始化为0，并逐渐的学习得到更大的权重

`在空间上做Attention module是将W和H乘起来作为N，在N上进行注意力的计算，如果是在channel上做的话,则是保留下C*C作为注意力矩阵`

2. **Channel Attention Module**

<img src="https://pic3.zhimg.com/80/v2-b282a763dc1b62cc0615c0a2a8e0beba_720w.jpg" alt="img" style="zoom:80%;" />

- 分别对**A**做reshape(CxN)和reshape与transpose(NxC)
- 将得到的两个特征图相乘，再通过softmax得到channel attention map **X**(C×C)
- 接着把**X**的转置(CxC)与reshape的**A**(CxN)做矩阵乘法，再乘以尺度系数β，再reshape为原来形状，最后与**A**相加得到最后的输出**E**
- 其中β初始化为0，并逐渐的学习得到更大的权重



### 6. 分割网络模型DeepLab

在图像分割中需要对每一个像素作出分类，换言之，语义分割是在像素级别理解图像。

<img src="https://image.jiqizhixin.com/uploads/editor/823f370d-1b53-48f5-81de-2866b3f26d66/1522033430979.jpg" alt="img" style="zoom: 50%;" />

**语义分割与实例分割**的区别。(中) 虽然它们是相同的目标，但它们被分类为不同的目标（实例分割）。(右) 相同的目标，相同的类别（语义分割）。

<img src="https://image.jiqizhixin.com/uploads/editor/9174de92-cec2-47e3-95b4-f19434cd8164/1522033431606.jpg" alt="img" style="zoom:50%;" />

*用于密集预测的全卷积神经网络。请注意，不存在池化层和全连接层。*

这个模型可以输出形状为 [W,H,C] 的概率张量，其中 W 和 H 代表的是宽度和高度，C 代表的是类别标签的个数。在第三个维度上应用最大化函数会得到形状为 [W,H,1] 的张量。然后，我们计算真实图像和我们的预测的每个像素之间的交叉熵。最终，我们对计算结果取平均值，并且使用反向传播算法训练网络。

1$\times$1卷积核的用处主要是降维和升维，例如6$\times$6$\times$32的特征图，经过1$\times$1$\times$32的卷积核之后变成6$\times$6$\times$1的特征图

 **① deeplab v1**

就物体检测的这个目标来说，将VGG-16中的FC层使用1*1卷积替代，变成全卷积网络即可解决

由于卷积神经网络在提取特征时会将输入图像逐渐缩小，featuremap变小形成高级别的特征对分割任务并不适用，DeepLab采用了**空洞卷积**替换传统的卷积和**fully connected CRF**。为了利用已经训练好的VGG模型进行fine-tuning，又想改变网络结构得到更加dense的score map而引入的空洞卷积。

CRF Conditional Random Field（条件随机场）在图像处理领域的作用是平滑处理，在针对每个位置的像素值处理时会考虑周围像素的值，采用fully connected CRF可以综合考虑全局信息，**恢复详细的局部结构**。

<img src="https://ask.qcloudimg.com/http-save/yehe-7336036/b5fe8xv6sw.png?imageView2/2/w/1620" alt="img" style="zoom: 67%;" />

**② deeplab v2**    《DeepLab-v2: Semantic Image Segmentation 》

DeepLabV2 在前一代的基础上进行改进，依旧是使用空洞卷积，但主干网络已经替换为ResNet，另外提出了空洞空间金字塔池化(ASPP)结构

<img src="https://pic3.zhimg.com/80/v2-3eb33e9c374d0de702804bb05250cc8a_720w.jpg" alt="img" style="zoom:67%;" />

其通过在固定的特征图上，使用不同空洞率的空洞卷积并行提取特征，能多比例捕获对象及上下文信息。

**③ deeplab v3**

v3版本去掉了最后的fully connected field，为了解决多尺度分割对象的问题，我们设计了采用级联或并行多个不同膨胀系数的空洞卷积模块，以更好的捕获上下文语义信息。此外，改进了在DeepLab V2中提出的ASPP模块，将**BN层引入ASPP**，采用**全局平均池化**进一步提升了它的性能，deeplabv3中，使用大采样率的3X3空洞卷积，图像边界响应无法捕捉远距离信息，会退化为1×1的卷积, 所以deeplabv3将图像级特征融合到ASPP模块中，**去掉了 CRFs**。
